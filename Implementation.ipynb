{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>40367.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Male</td>\n",
       "      <td>54</td>\n",
       "      <td>45084.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>52483.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>40941.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>50289.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number    City Gender  Age   Income Illness\n",
       "0       1  Dallas   Male   41  40367.0      No\n",
       "1       2  Dallas   Male   54  45084.0      No\n",
       "2       3  Dallas   Male   42  52483.0      No\n",
       "3       4  Dallas   Male   40  40941.0      No\n",
       "4       5  Dallas   Male   46  50289.0      No"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "dataset = pd.read_csv(\"dataset/toy_dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the Number column\n",
    "dataset = dataset.drop(['Number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using custom label encoding to convert the categorical data to numerical data\n",
    "from mlmodule.preprocessing import LabelEncoder\n",
    "\n",
    "#City column\n",
    "city_le = LabelEncoder()\n",
    "dataset['City'] = city_le.fit_transform(dataset['City'])\n",
    "\n",
    "#Gender column\n",
    "gender_le = LabelEncoder()\n",
    "dataset['Gender'] = gender_le.fit_transform(dataset['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>40367.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>45084.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>52483.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40941.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>50289.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   City  Gender  Age   Income Illness\n",
       "0     2       1   41  40367.0      No\n",
       "1     2       1   54  45084.0      No\n",
       "2     2       1   42  52483.0      No\n",
       "3     2       1   40  40941.0      No\n",
       "4     2       1   46  50289.0      No"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from mlmodule.preprocessing import StandardScaler\n",
    "numerical_columns = ['City', 'Gender', 'Age', 'Income']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset[numerical_columns] = scaler.fit_transform(dataset[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.869701</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>-0.341072</td>\n",
       "      <td>-2.034076</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.869701</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>0.782161</td>\n",
       "      <td>-1.845478</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.869701</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>-0.254670</td>\n",
       "      <td>-1.549648</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.869701</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>-0.427475</td>\n",
       "      <td>-2.011126</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.869701</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>0.090940</td>\n",
       "      <td>-1.637369</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City    Gender       Age    Income Illness\n",
       "0 -0.869701  0.889214 -0.341072 -2.034076      No\n",
       "1 -0.869701  0.889214  0.782161 -1.845478      No\n",
       "2 -0.869701  0.889214 -0.254670 -1.549648      No\n",
       "3 -0.869701  0.889214 -0.427475 -2.011126      No\n",
       "4 -0.869701  0.889214  0.090940 -1.637369      No"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the illness column to 0 for No and 1 for Yes\n",
    "dataset['Illness'] = dataset['Illness'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illness 0:  (137202, 5)\n",
      "Illness 1:  (12130, 5)\n"
     ]
    }
   ],
   "source": [
    "dataset_illness_0 = dataset[dataset['Illness'] == 0]\n",
    "dataset_illness_1 = dataset[dataset['Illness'] == 1]\n",
    "print(\"Illness 0: \", dataset_illness_0.shape)\n",
    "print(\"Illness 1: \", dataset_illness_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_illness_0 = dataset_illness_0.sample(n=60000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two datasets\n",
    "dataset = pd.concat([dataset_illness_0, dataset_illness_1], axis=0)\n",
    "\n",
    "#Shuffling the dataset\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.963903</td>\n",
       "      <td>0.889214</td>\n",
       "      <td>0.350148</td>\n",
       "      <td>-0.315107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771603</td>\n",
       "      <td>-1.124588</td>\n",
       "      <td>-1.377903</td>\n",
       "      <td>-0.207674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.322600</td>\n",
       "      <td>-1.124588</td>\n",
       "      <td>0.350148</td>\n",
       "      <td>-0.124871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771603</td>\n",
       "      <td>-1.124588</td>\n",
       "      <td>0.522953</td>\n",
       "      <td>0.369154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224502</td>\n",
       "      <td>-1.124588</td>\n",
       "      <td>-0.859488</td>\n",
       "      <td>0.885088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City    Gender       Age    Income  Illness\n",
       "0 -1.963903  0.889214  0.350148 -0.315107        0\n",
       "1  0.771603 -1.124588 -1.377903 -0.207674        0\n",
       "2 -0.322600 -1.124588  0.350148 -0.124871        1\n",
       "3  0.771603 -1.124588  0.522953  0.369154        0\n",
       "4  0.224502 -1.124588 -0.859488  0.885088        0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "margin = int(0.8 * dataset.shape[0])\n",
    "X_train, X_test, y_train, y_test = dataset.iloc[:margin, :-1], dataset.iloc[margin:, :-1], dataset.iloc[:margin, -1], dataset.iloc[margin:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600, Loss: 0.4623\n",
      "Epoch 2/600, Loss: 0.4559\n",
      "Epoch 3/600, Loss: 0.4559\n",
      "Epoch 4/600, Loss: 0.4554\n",
      "Epoch 5/600, Loss: 0.4552\n",
      "Epoch 6/600, Loss: 0.4550\n",
      "Epoch 7/600, Loss: 0.4549\n",
      "Epoch 8/600, Loss: 0.4550\n",
      "Epoch 9/600, Loss: 0.4549\n",
      "Epoch 10/600, Loss: 0.4548\n",
      "Epoch 11/600, Loss: 0.4548\n",
      "Epoch 12/600, Loss: 0.4549\n",
      "Epoch 13/600, Loss: 0.4548\n",
      "Epoch 14/600, Loss: 0.4547\n",
      "Epoch 15/600, Loss: 0.4546\n",
      "Epoch 16/600, Loss: 0.4546\n",
      "Epoch 17/600, Loss: 0.4547\n",
      "Epoch 18/600, Loss: 0.4546\n",
      "Epoch 19/600, Loss: 0.4546\n",
      "Epoch 20/600, Loss: 0.4546\n",
      "Epoch 21/600, Loss: 0.4543\n",
      "Epoch 22/600, Loss: 0.4545\n",
      "Epoch 23/600, Loss: 0.4544\n",
      "Epoch 24/600, Loss: 0.4545\n",
      "Epoch 25/600, Loss: 0.4545\n",
      "Epoch 26/600, Loss: 0.4545\n",
      "Epoch 27/600, Loss: 0.4544\n",
      "Epoch 28/600, Loss: 0.4544\n",
      "Epoch 29/600, Loss: 0.4544\n",
      "Epoch 30/600, Loss: 0.4543\n",
      "Epoch 31/600, Loss: 0.4543\n",
      "Epoch 32/600, Loss: 0.4543\n",
      "Epoch 33/600, Loss: 0.4542\n",
      "Epoch 34/600, Loss: 0.4543\n",
      "Epoch 35/600, Loss: 0.4543\n",
      "Epoch 36/600, Loss: 0.4543\n",
      "Epoch 37/600, Loss: 0.4543\n",
      "Epoch 38/600, Loss: 0.4542\n",
      "Epoch 39/600, Loss: 0.4541\n",
      "Epoch 40/600, Loss: 0.4542\n",
      "Epoch 41/600, Loss: 0.4543\n",
      "Epoch 42/600, Loss: 0.4541\n",
      "Epoch 43/600, Loss: 0.4540\n",
      "Epoch 44/600, Loss: 0.4541\n",
      "Epoch 45/600, Loss: 0.4539\n",
      "Epoch 46/600, Loss: 0.4540\n",
      "Epoch 47/600, Loss: 0.4540\n",
      "Epoch 48/600, Loss: 0.4540\n",
      "Epoch 49/600, Loss: 0.4538\n",
      "Epoch 50/600, Loss: 0.4540\n",
      "Epoch 51/600, Loss: 0.4540\n",
      "Epoch 52/600, Loss: 0.4539\n",
      "Epoch 53/600, Loss: 0.4538\n",
      "Epoch 54/600, Loss: 0.4539\n",
      "Epoch 55/600, Loss: 0.4538\n",
      "Epoch 56/600, Loss: 0.4541\n",
      "Epoch 57/600, Loss: 0.4538\n",
      "Epoch 58/600, Loss: 0.4540\n",
      "Epoch 59/600, Loss: 0.4538\n",
      "Epoch 60/600, Loss: 0.4537\n",
      "Epoch 61/600, Loss: 0.4537\n",
      "Epoch 62/600, Loss: 0.4537\n",
      "Epoch 63/600, Loss: 0.4538\n",
      "Epoch 64/600, Loss: 0.4537\n",
      "Epoch 65/600, Loss: 0.4537\n",
      "Epoch 66/600, Loss: 0.4537\n",
      "Epoch 67/600, Loss: 0.4536\n",
      "Epoch 68/600, Loss: 0.4536\n",
      "Epoch 69/600, Loss: 0.4537\n",
      "Epoch 70/600, Loss: 0.4538\n",
      "Epoch 71/600, Loss: 0.4535\n",
      "Epoch 72/600, Loss: 0.4537\n",
      "Epoch 73/600, Loss: 0.4535\n",
      "Epoch 74/600, Loss: 0.4535\n",
      "Epoch 75/600, Loss: 0.4535\n",
      "Epoch 76/600, Loss: 0.4536\n",
      "Epoch 77/600, Loss: 0.4534\n",
      "Epoch 78/600, Loss: 0.4535\n",
      "Epoch 79/600, Loss: 0.4534\n",
      "Epoch 80/600, Loss: 0.4534\n",
      "Epoch 81/600, Loss: 0.4534\n",
      "Epoch 82/600, Loss: 0.4534\n",
      "Epoch 83/600, Loss: 0.4535\n",
      "Epoch 84/600, Loss: 0.4534\n",
      "Epoch 85/600, Loss: 0.4535\n",
      "Epoch 86/600, Loss: 0.4534\n",
      "Epoch 87/600, Loss: 0.4533\n",
      "Epoch 88/600, Loss: 0.4534\n",
      "Epoch 89/600, Loss: 0.4533\n",
      "Epoch 90/600, Loss: 0.4532\n",
      "Epoch 91/600, Loss: 0.4532\n",
      "Epoch 92/600, Loss: 0.4534\n",
      "Epoch 93/600, Loss: 0.4533\n",
      "Epoch 94/600, Loss: 0.4533\n",
      "Epoch 95/600, Loss: 0.4532\n",
      "Epoch 96/600, Loss: 0.4533\n",
      "Epoch 97/600, Loss: 0.4531\n",
      "Epoch 98/600, Loss: 0.4531\n",
      "Epoch 99/600, Loss: 0.4532\n",
      "Epoch 100/600, Loss: 0.4531\n",
      "Epoch 101/600, Loss: 0.4532\n",
      "Epoch 102/600, Loss: 0.4531\n",
      "Epoch 103/600, Loss: 0.4531\n",
      "Epoch 104/600, Loss: 0.4530\n",
      "Epoch 105/600, Loss: 0.4531\n",
      "Epoch 106/600, Loss: 0.4530\n",
      "Epoch 107/600, Loss: 0.4530\n",
      "Epoch 108/600, Loss: 0.4530\n",
      "Epoch 109/600, Loss: 0.4530\n",
      "Epoch 110/600, Loss: 0.4530\n",
      "Epoch 111/600, Loss: 0.4529\n",
      "Epoch 112/600, Loss: 0.4528\n",
      "Epoch 113/600, Loss: 0.4529\n",
      "Epoch 114/600, Loss: 0.4531\n",
      "Epoch 115/600, Loss: 0.4529\n",
      "Epoch 116/600, Loss: 0.4529\n",
      "Epoch 117/600, Loss: 0.4530\n",
      "Epoch 118/600, Loss: 0.4528\n",
      "Epoch 119/600, Loss: 0.4528\n",
      "Epoch 120/600, Loss: 0.4528\n",
      "Epoch 121/600, Loss: 0.4528\n",
      "Epoch 122/600, Loss: 0.4528\n",
      "Epoch 123/600, Loss: 0.4527\n",
      "Epoch 124/600, Loss: 0.4527\n",
      "Epoch 125/600, Loss: 0.4528\n",
      "Epoch 126/600, Loss: 0.4527\n",
      "Epoch 127/600, Loss: 0.4527\n",
      "Epoch 128/600, Loss: 0.4529\n",
      "Epoch 129/600, Loss: 0.4527\n",
      "Epoch 130/600, Loss: 0.4527\n",
      "Epoch 131/600, Loss: 0.4529\n",
      "Epoch 132/600, Loss: 0.4528\n",
      "Epoch 133/600, Loss: 0.4526\n",
      "Epoch 134/600, Loss: 0.4527\n",
      "Epoch 135/600, Loss: 0.4527\n",
      "Epoch 136/600, Loss: 0.4524\n",
      "Epoch 137/600, Loss: 0.4525\n",
      "Epoch 138/600, Loss: 0.4525\n",
      "Epoch 139/600, Loss: 0.4528\n",
      "Epoch 140/600, Loss: 0.4525\n",
      "Epoch 141/600, Loss: 0.4525\n",
      "Epoch 142/600, Loss: 0.4524\n",
      "Epoch 143/600, Loss: 0.4525\n",
      "Epoch 144/600, Loss: 0.4524\n",
      "Epoch 145/600, Loss: 0.4526\n",
      "Epoch 146/600, Loss: 0.4525\n",
      "Epoch 147/600, Loss: 0.4524\n",
      "Epoch 148/600, Loss: 0.4525\n",
      "Epoch 149/600, Loss: 0.4524\n",
      "Epoch 150/600, Loss: 0.4525\n",
      "Epoch 151/600, Loss: 0.4525\n",
      "Epoch 152/600, Loss: 0.4523\n",
      "Epoch 153/600, Loss: 0.4524\n",
      "Epoch 154/600, Loss: 0.4523\n",
      "Epoch 155/600, Loss: 0.4524\n",
      "Epoch 156/600, Loss: 0.4523\n",
      "Epoch 157/600, Loss: 0.4523\n",
      "Epoch 158/600, Loss: 0.4523\n",
      "Epoch 159/600, Loss: 0.4524\n",
      "Epoch 160/600, Loss: 0.4523\n",
      "Epoch 161/600, Loss: 0.4521\n",
      "Epoch 162/600, Loss: 0.4523\n",
      "Epoch 163/600, Loss: 0.4521\n",
      "Epoch 164/600, Loss: 0.4522\n",
      "Epoch 165/600, Loss: 0.4522\n",
      "Epoch 166/600, Loss: 0.4521\n",
      "Epoch 167/600, Loss: 0.4523\n",
      "Epoch 168/600, Loss: 0.4520\n",
      "Epoch 169/600, Loss: 0.4522\n",
      "Epoch 170/600, Loss: 0.4521\n",
      "Epoch 171/600, Loss: 0.4520\n",
      "Epoch 172/600, Loss: 0.4521\n",
      "Epoch 173/600, Loss: 0.4520\n",
      "Epoch 174/600, Loss: 0.4520\n",
      "Epoch 175/600, Loss: 0.4521\n",
      "Epoch 176/600, Loss: 0.4520\n",
      "Epoch 177/600, Loss: 0.4518\n",
      "Epoch 178/600, Loss: 0.4521\n",
      "Epoch 179/600, Loss: 0.4519\n",
      "Epoch 180/600, Loss: 0.4520\n",
      "Epoch 181/600, Loss: 0.4520\n",
      "Epoch 182/600, Loss: 0.4520\n",
      "Epoch 183/600, Loss: 0.4519\n",
      "Epoch 184/600, Loss: 0.4519\n",
      "Epoch 185/600, Loss: 0.4519\n",
      "Epoch 186/600, Loss: 0.4519\n",
      "Epoch 187/600, Loss: 0.4519\n",
      "Epoch 188/600, Loss: 0.4518\n",
      "Epoch 189/600, Loss: 0.4517\n",
      "Epoch 190/600, Loss: 0.4518\n",
      "Epoch 191/600, Loss: 0.4518\n",
      "Epoch 192/600, Loss: 0.4518\n",
      "Epoch 193/600, Loss: 0.4515\n",
      "Epoch 194/600, Loss: 0.4518\n",
      "Epoch 195/600, Loss: 0.4517\n",
      "Epoch 196/600, Loss: 0.4517\n",
      "Epoch 197/600, Loss: 0.4516\n",
      "Epoch 198/600, Loss: 0.4515\n",
      "Epoch 199/600, Loss: 0.4515\n",
      "Epoch 200/600, Loss: 0.4516\n",
      "Epoch 201/600, Loss: 0.4516\n",
      "Epoch 202/600, Loss: 0.4516\n",
      "Epoch 203/600, Loss: 0.4515\n",
      "Epoch 204/600, Loss: 0.4516\n",
      "Epoch 205/600, Loss: 0.4517\n",
      "Epoch 206/600, Loss: 0.4515\n",
      "Epoch 207/600, Loss: 0.4513\n",
      "Epoch 208/600, Loss: 0.4514\n",
      "Epoch 209/600, Loss: 0.4515\n",
      "Epoch 210/600, Loss: 0.4514\n",
      "Epoch 211/600, Loss: 0.4513\n",
      "Epoch 212/600, Loss: 0.4514\n",
      "Epoch 213/600, Loss: 0.4514\n",
      "Epoch 214/600, Loss: 0.4513\n",
      "Epoch 215/600, Loss: 0.4516\n",
      "Epoch 216/600, Loss: 0.4516\n",
      "Epoch 217/600, Loss: 0.4513\n",
      "Epoch 218/600, Loss: 0.4515\n",
      "Epoch 219/600, Loss: 0.4514\n",
      "Epoch 220/600, Loss: 0.4513\n",
      "Epoch 221/600, Loss: 0.4513\n",
      "Epoch 222/600, Loss: 0.4513\n",
      "Epoch 223/600, Loss: 0.4513\n",
      "Epoch 224/600, Loss: 0.4514\n",
      "Epoch 225/600, Loss: 0.4514\n",
      "Epoch 226/600, Loss: 0.4512\n",
      "Epoch 227/600, Loss: 0.4512\n",
      "Epoch 228/600, Loss: 0.4512\n",
      "Epoch 229/600, Loss: 0.4510\n",
      "Epoch 230/600, Loss: 0.4513\n",
      "Epoch 231/600, Loss: 0.4513\n",
      "Epoch 232/600, Loss: 0.4513\n",
      "Epoch 233/600, Loss: 0.4513\n",
      "Epoch 234/600, Loss: 0.4511\n",
      "Epoch 235/600, Loss: 0.4511\n",
      "Epoch 236/600, Loss: 0.4512\n",
      "Epoch 237/600, Loss: 0.4512\n",
      "Epoch 238/600, Loss: 0.4512\n",
      "Epoch 239/600, Loss: 0.4511\n",
      "Epoch 240/600, Loss: 0.4512\n",
      "Epoch 241/600, Loss: 0.4511\n",
      "Epoch 242/600, Loss: 0.4509\n",
      "Epoch 243/600, Loss: 0.4512\n",
      "Epoch 244/600, Loss: 0.4510\n",
      "Epoch 245/600, Loss: 0.4510\n",
      "Epoch 246/600, Loss: 0.4509\n",
      "Epoch 247/600, Loss: 0.4509\n",
      "Epoch 248/600, Loss: 0.4509\n",
      "Epoch 249/600, Loss: 0.4509\n",
      "Epoch 250/600, Loss: 0.4509\n",
      "Epoch 251/600, Loss: 0.4511\n",
      "Epoch 252/600, Loss: 0.4510\n",
      "Epoch 253/600, Loss: 0.4512\n",
      "Epoch 254/600, Loss: 0.4509\n",
      "Epoch 255/600, Loss: 0.4511\n",
      "Epoch 256/600, Loss: 0.4509\n",
      "Epoch 257/600, Loss: 0.4509\n",
      "Epoch 258/600, Loss: 0.4509\n",
      "Epoch 259/600, Loss: 0.4507\n",
      "Epoch 260/600, Loss: 0.4509\n",
      "Epoch 261/600, Loss: 0.4507\n",
      "Epoch 262/600, Loss: 0.4510\n",
      "Epoch 263/600, Loss: 0.4508\n",
      "Epoch 264/600, Loss: 0.4508\n",
      "Epoch 265/600, Loss: 0.4508\n",
      "Epoch 266/600, Loss: 0.4508\n",
      "Epoch 267/600, Loss: 0.4507\n",
      "Epoch 268/600, Loss: 0.4510\n",
      "Epoch 269/600, Loss: 0.4506\n",
      "Epoch 270/600, Loss: 0.4507\n",
      "Epoch 271/600, Loss: 0.4509\n",
      "Epoch 272/600, Loss: 0.4505\n",
      "Epoch 273/600, Loss: 0.4508\n",
      "Epoch 274/600, Loss: 0.4507\n",
      "Epoch 275/600, Loss: 0.4509\n",
      "Epoch 276/600, Loss: 0.4506\n",
      "Epoch 277/600, Loss: 0.4506\n",
      "Epoch 278/600, Loss: 0.4506\n",
      "Epoch 279/600, Loss: 0.4508\n",
      "Epoch 280/600, Loss: 0.4506\n",
      "Epoch 281/600, Loss: 0.4506\n",
      "Epoch 282/600, Loss: 0.4509\n",
      "Epoch 283/600, Loss: 0.4507\n",
      "Epoch 284/600, Loss: 0.4506\n",
      "Epoch 285/600, Loss: 0.4508\n",
      "Epoch 286/600, Loss: 0.4504\n",
      "Epoch 287/600, Loss: 0.4506\n",
      "Epoch 288/600, Loss: 0.4505\n",
      "Epoch 289/600, Loss: 0.4503\n",
      "Epoch 290/600, Loss: 0.4505\n",
      "Epoch 291/600, Loss: 0.4506\n",
      "Epoch 292/600, Loss: 0.4507\n",
      "Epoch 293/600, Loss: 0.4504\n",
      "Epoch 294/600, Loss: 0.4504\n",
      "Epoch 295/600, Loss: 0.4506\n",
      "Epoch 296/600, Loss: 0.4505\n",
      "Epoch 297/600, Loss: 0.4505\n",
      "Epoch 298/600, Loss: 0.4504\n",
      "Epoch 299/600, Loss: 0.4503\n",
      "Epoch 300/600, Loss: 0.4503\n",
      "Epoch 301/600, Loss: 0.4503\n",
      "Epoch 302/600, Loss: 0.4502\n",
      "Epoch 303/600, Loss: 0.4503\n",
      "Epoch 304/600, Loss: 0.4503\n",
      "Epoch 305/600, Loss: 0.4502\n",
      "Epoch 306/600, Loss: 0.4502\n",
      "Epoch 307/600, Loss: 0.4503\n",
      "Epoch 308/600, Loss: 0.4502\n",
      "Epoch 309/600, Loss: 0.4505\n",
      "Epoch 310/600, Loss: 0.4506\n",
      "Epoch 311/600, Loss: 0.4501\n",
      "Epoch 312/600, Loss: 0.4501\n",
      "Epoch 313/600, Loss: 0.4505\n",
      "Epoch 314/600, Loss: 0.4502\n",
      "Epoch 315/600, Loss: 0.4501\n",
      "Epoch 316/600, Loss: 0.4504\n",
      "Epoch 317/600, Loss: 0.4502\n",
      "Epoch 318/600, Loss: 0.4503\n",
      "Epoch 319/600, Loss: 0.4501\n",
      "Epoch 320/600, Loss: 0.4502\n",
      "Epoch 321/600, Loss: 0.4501\n",
      "Epoch 322/600, Loss: 0.4502\n",
      "Epoch 323/600, Loss: 0.4502\n",
      "Epoch 324/600, Loss: 0.4502\n",
      "Epoch 325/600, Loss: 0.4502\n",
      "Epoch 326/600, Loss: 0.4500\n",
      "Epoch 327/600, Loss: 0.4503\n",
      "Epoch 328/600, Loss: 0.4504\n",
      "Epoch 329/600, Loss: 0.4500\n",
      "Epoch 330/600, Loss: 0.4502\n",
      "Epoch 331/600, Loss: 0.4500\n",
      "Epoch 332/600, Loss: 0.4499\n",
      "Epoch 333/600, Loss: 0.4503\n",
      "Epoch 334/600, Loss: 0.4500\n",
      "Epoch 335/600, Loss: 0.4500\n",
      "Epoch 336/600, Loss: 0.4499\n",
      "Epoch 337/600, Loss: 0.4502\n",
      "Epoch 338/600, Loss: 0.4502\n",
      "Epoch 339/600, Loss: 0.4501\n",
      "Epoch 340/600, Loss: 0.4499\n",
      "Epoch 341/600, Loss: 0.4500\n",
      "Epoch 342/600, Loss: 0.4499\n",
      "Epoch 343/600, Loss: 0.4500\n",
      "Epoch 344/600, Loss: 0.4501\n",
      "Epoch 345/600, Loss: 0.4502\n",
      "Epoch 346/600, Loss: 0.4501\n",
      "Epoch 347/600, Loss: 0.4500\n",
      "Epoch 348/600, Loss: 0.4500\n",
      "Epoch 349/600, Loss: 0.4499\n",
      "Epoch 350/600, Loss: 0.4498\n",
      "Epoch 351/600, Loss: 0.4500\n",
      "Epoch 352/600, Loss: 0.4501\n",
      "Epoch 353/600, Loss: 0.4501\n",
      "Epoch 354/600, Loss: 0.4498\n",
      "Epoch 355/600, Loss: 0.4499\n",
      "Epoch 356/600, Loss: 0.4499\n",
      "Epoch 357/600, Loss: 0.4500\n",
      "Epoch 358/600, Loss: 0.4502\n",
      "Epoch 359/600, Loss: 0.4499\n",
      "Epoch 360/600, Loss: 0.4500\n",
      "Epoch 361/600, Loss: 0.4499\n",
      "Epoch 362/600, Loss: 0.4499\n",
      "Epoch 363/600, Loss: 0.4499\n",
      "Epoch 364/600, Loss: 0.4498\n",
      "Epoch 365/600, Loss: 0.4499\n",
      "Epoch 366/600, Loss: 0.4497\n",
      "Epoch 367/600, Loss: 0.4497\n",
      "Epoch 368/600, Loss: 0.4500\n",
      "Epoch 369/600, Loss: 0.4499\n",
      "Epoch 370/600, Loss: 0.4498\n",
      "Epoch 371/600, Loss: 0.4499\n",
      "Epoch 372/600, Loss: 0.4497\n",
      "Epoch 373/600, Loss: 0.4497\n",
      "Epoch 374/600, Loss: 0.4499\n",
      "Epoch 375/600, Loss: 0.4498\n",
      "Epoch 376/600, Loss: 0.4497\n",
      "Epoch 377/600, Loss: 0.4495\n",
      "Epoch 378/600, Loss: 0.4499\n",
      "Epoch 379/600, Loss: 0.4496\n",
      "Epoch 380/600, Loss: 0.4497\n",
      "Epoch 381/600, Loss: 0.4496\n",
      "Epoch 382/600, Loss: 0.4497\n",
      "Epoch 383/600, Loss: 0.4497\n",
      "Epoch 384/600, Loss: 0.4495\n",
      "Epoch 385/600, Loss: 0.4495\n",
      "Epoch 386/600, Loss: 0.4497\n",
      "Epoch 387/600, Loss: 0.4496\n",
      "Epoch 388/600, Loss: 0.4495\n",
      "Epoch 389/600, Loss: 0.4495\n",
      "Epoch 390/600, Loss: 0.4493\n",
      "Epoch 391/600, Loss: 0.4495\n",
      "Epoch 392/600, Loss: 0.4495\n",
      "Epoch 393/600, Loss: 0.4498\n",
      "Epoch 394/600, Loss: 0.4494\n",
      "Epoch 395/600, Loss: 0.4497\n",
      "Epoch 396/600, Loss: 0.4497\n",
      "Epoch 397/600, Loss: 0.4494\n",
      "Epoch 398/600, Loss: 0.4496\n",
      "Epoch 399/600, Loss: 0.4500\n",
      "Epoch 400/600, Loss: 0.4494\n",
      "Epoch 401/600, Loss: 0.4496\n",
      "Epoch 402/600, Loss: 0.4494\n",
      "Epoch 403/600, Loss: 0.4496\n",
      "Epoch 404/600, Loss: 0.4493\n",
      "Epoch 405/600, Loss: 0.4498\n",
      "Epoch 406/600, Loss: 0.4497\n",
      "Epoch 407/600, Loss: 0.4497\n",
      "Epoch 408/600, Loss: 0.4494\n",
      "Epoch 409/600, Loss: 0.4494\n",
      "Epoch 410/600, Loss: 0.4494\n",
      "Epoch 411/600, Loss: 0.4495\n",
      "Epoch 412/600, Loss: 0.4493\n",
      "Epoch 413/600, Loss: 0.4494\n",
      "Epoch 414/600, Loss: 0.4494\n",
      "Epoch 415/600, Loss: 0.4496\n",
      "Epoch 416/600, Loss: 0.4494\n",
      "Epoch 417/600, Loss: 0.4493\n",
      "Epoch 418/600, Loss: 0.4495\n",
      "Epoch 419/600, Loss: 0.4493\n",
      "Epoch 420/600, Loss: 0.4496\n",
      "Epoch 421/600, Loss: 0.4494\n",
      "Epoch 422/600, Loss: 0.4493\n",
      "Epoch 423/600, Loss: 0.4492\n",
      "Epoch 424/600, Loss: 0.4495\n",
      "Epoch 425/600, Loss: 0.4491\n",
      "Epoch 426/600, Loss: 0.4496\n",
      "Epoch 427/600, Loss: 0.4494\n",
      "Epoch 428/600, Loss: 0.4492\n",
      "Epoch 429/600, Loss: 0.4492\n",
      "Epoch 430/600, Loss: 0.4492\n",
      "Epoch 431/600, Loss: 0.4493\n",
      "Epoch 432/600, Loss: 0.4493\n",
      "Epoch 433/600, Loss: 0.4492\n",
      "Epoch 434/600, Loss: 0.4494\n",
      "Epoch 435/600, Loss: 0.4492\n",
      "Epoch 436/600, Loss: 0.4493\n",
      "Epoch 437/600, Loss: 0.4492\n",
      "Epoch 438/600, Loss: 0.4492\n",
      "Epoch 439/600, Loss: 0.4489\n",
      "Epoch 440/600, Loss: 0.4493\n",
      "Epoch 441/600, Loss: 0.4498\n",
      "Epoch 442/600, Loss: 0.4492\n",
      "Epoch 443/600, Loss: 0.4493\n",
      "Epoch 444/600, Loss: 0.4492\n",
      "Epoch 445/600, Loss: 0.4492\n",
      "Epoch 446/600, Loss: 0.4489\n",
      "Epoch 447/600, Loss: 0.4492\n",
      "Epoch 448/600, Loss: 0.4489\n",
      "Epoch 449/600, Loss: 0.4491\n",
      "Epoch 450/600, Loss: 0.4491\n",
      "Epoch 451/600, Loss: 0.4491\n",
      "Epoch 452/600, Loss: 0.4493\n",
      "Epoch 453/600, Loss: 0.4492\n",
      "Epoch 454/600, Loss: 0.4492\n",
      "Epoch 455/600, Loss: 0.4491\n",
      "Epoch 456/600, Loss: 0.4489\n",
      "Epoch 457/600, Loss: 0.4489\n",
      "Epoch 458/600, Loss: 0.4491\n",
      "Epoch 459/600, Loss: 0.4490\n",
      "Epoch 460/600, Loss: 0.4490\n",
      "Epoch 461/600, Loss: 0.4496\n",
      "Epoch 462/600, Loss: 0.4493\n",
      "Epoch 463/600, Loss: 0.4489\n",
      "Epoch 464/600, Loss: 0.4491\n",
      "Epoch 465/600, Loss: 0.4491\n",
      "Epoch 466/600, Loss: 0.4490\n",
      "Epoch 467/600, Loss: 0.4490\n",
      "Epoch 468/600, Loss: 0.4489\n",
      "Epoch 469/600, Loss: 0.4489\n",
      "Epoch 470/600, Loss: 0.4490\n",
      "Epoch 471/600, Loss: 0.4489\n",
      "Epoch 472/600, Loss: 0.4489\n",
      "Epoch 473/600, Loss: 0.4490\n",
      "Epoch 474/600, Loss: 0.4491\n",
      "Epoch 475/600, Loss: 0.4493\n",
      "Epoch 476/600, Loss: 0.4490\n",
      "Epoch 477/600, Loss: 0.4490\n",
      "Epoch 478/600, Loss: 0.4489\n",
      "Epoch 479/600, Loss: 0.4492\n",
      "Epoch 480/600, Loss: 0.4485\n",
      "Epoch 481/600, Loss: 0.4491\n",
      "Epoch 482/600, Loss: 0.4489\n",
      "Epoch 483/600, Loss: 0.4488\n",
      "Epoch 484/600, Loss: 0.4492\n",
      "Epoch 485/600, Loss: 0.4491\n",
      "Epoch 486/600, Loss: 0.4492\n",
      "Epoch 487/600, Loss: 0.4490\n",
      "Epoch 488/600, Loss: 0.4491\n",
      "Epoch 489/600, Loss: 0.4492\n",
      "Epoch 490/600, Loss: 0.4488\n",
      "Epoch 491/600, Loss: 0.4488\n",
      "Epoch 492/600, Loss: 0.4487\n",
      "Epoch 493/600, Loss: 0.4488\n",
      "Epoch 494/600, Loss: 0.4488\n",
      "Epoch 495/600, Loss: 0.4487\n",
      "Epoch 496/600, Loss: 0.4489\n",
      "Epoch 497/600, Loss: 0.4490\n",
      "Epoch 498/600, Loss: 0.4486\n",
      "Epoch 499/600, Loss: 0.4490\n",
      "Epoch 500/600, Loss: 0.4487\n",
      "Epoch 501/600, Loss: 0.4488\n",
      "Epoch 502/600, Loss: 0.4487\n",
      "Epoch 503/600, Loss: 0.4486\n",
      "Epoch 504/600, Loss: 0.4487\n",
      "Epoch 505/600, Loss: 0.4485\n",
      "Epoch 506/600, Loss: 0.4489\n",
      "Epoch 507/600, Loss: 0.4490\n",
      "Epoch 508/600, Loss: 0.4487\n",
      "Epoch 509/600, Loss: 0.4491\n",
      "Epoch 510/600, Loss: 0.4486\n",
      "Epoch 511/600, Loss: 0.4488\n",
      "Epoch 512/600, Loss: 0.4485\n",
      "Epoch 513/600, Loss: 0.4487\n",
      "Epoch 514/600, Loss: 0.4485\n",
      "Epoch 515/600, Loss: 0.4486\n",
      "Epoch 516/600, Loss: 0.4484\n",
      "Epoch 517/600, Loss: 0.4486\n",
      "Epoch 518/600, Loss: 0.4485\n",
      "Epoch 519/600, Loss: 0.4484\n",
      "Epoch 520/600, Loss: 0.4491\n",
      "Epoch 521/600, Loss: 0.4490\n",
      "Epoch 522/600, Loss: 0.4489\n",
      "Epoch 523/600, Loss: 0.4489\n",
      "Epoch 524/600, Loss: 0.4487\n",
      "Epoch 525/600, Loss: 0.4486\n",
      "Epoch 526/600, Loss: 0.4486\n",
      "Epoch 527/600, Loss: 0.4487\n",
      "Epoch 528/600, Loss: 0.4485\n",
      "Epoch 529/600, Loss: 0.4487\n",
      "Epoch 530/600, Loss: 0.4484\n",
      "Epoch 531/600, Loss: 0.4485\n",
      "Epoch 532/600, Loss: 0.4485\n",
      "Epoch 533/600, Loss: 0.4485\n",
      "Epoch 534/600, Loss: 0.4485\n",
      "Epoch 535/600, Loss: 0.4489\n",
      "Epoch 536/600, Loss: 0.4489\n",
      "Epoch 537/600, Loss: 0.4488\n",
      "Epoch 538/600, Loss: 0.4486\n",
      "Epoch 539/600, Loss: 0.4486\n",
      "Epoch 540/600, Loss: 0.4489\n",
      "Epoch 541/600, Loss: 0.4485\n",
      "Epoch 542/600, Loss: 0.4484\n",
      "Epoch 543/600, Loss: 0.4485\n",
      "Epoch 544/600, Loss: 0.4486\n",
      "Epoch 545/600, Loss: 0.4486\n",
      "Epoch 546/600, Loss: 0.4484\n",
      "Epoch 547/600, Loss: 0.4484\n",
      "Epoch 548/600, Loss: 0.4486\n",
      "Epoch 549/600, Loss: 0.4483\n",
      "Epoch 550/600, Loss: 0.4485\n",
      "Epoch 551/600, Loss: 0.4485\n",
      "Epoch 552/600, Loss: 0.4483\n",
      "Epoch 553/600, Loss: 0.4484\n",
      "Epoch 554/600, Loss: 0.4484\n",
      "Epoch 555/600, Loss: 0.4485\n",
      "Epoch 556/600, Loss: 0.4483\n",
      "Epoch 557/600, Loss: 0.4484\n",
      "Epoch 558/600, Loss: 0.4486\n",
      "Epoch 559/600, Loss: 0.4484\n",
      "Epoch 560/600, Loss: 0.4485\n",
      "Epoch 561/600, Loss: 0.4484\n",
      "Epoch 562/600, Loss: 0.4483\n",
      "Epoch 563/600, Loss: 0.4483\n",
      "Epoch 564/600, Loss: 0.4485\n",
      "Epoch 565/600, Loss: 0.4483\n",
      "Epoch 566/600, Loss: 0.4487\n",
      "Epoch 567/600, Loss: 0.4482\n",
      "Epoch 568/600, Loss: 0.4485\n",
      "Epoch 569/600, Loss: 0.4484\n",
      "Epoch 570/600, Loss: 0.4483\n",
      "Epoch 571/600, Loss: 0.4486\n",
      "Epoch 572/600, Loss: 0.4486\n",
      "Epoch 573/600, Loss: 0.4483\n",
      "Epoch 574/600, Loss: 0.4489\n",
      "Epoch 575/600, Loss: 0.4485\n",
      "Epoch 576/600, Loss: 0.4485\n",
      "Epoch 577/600, Loss: 0.4483\n",
      "Epoch 578/600, Loss: 0.4482\n",
      "Epoch 579/600, Loss: 0.4484\n",
      "Epoch 580/600, Loss: 0.4483\n",
      "Epoch 581/600, Loss: 0.4483\n",
      "Epoch 582/600, Loss: 0.4484\n",
      "Epoch 583/600, Loss: 0.4485\n",
      "Epoch 584/600, Loss: 0.4487\n",
      "Epoch 585/600, Loss: 0.4485\n",
      "Epoch 586/600, Loss: 0.4482\n",
      "Epoch 587/600, Loss: 0.4484\n",
      "Epoch 588/600, Loss: 0.4484\n",
      "Epoch 589/600, Loss: 0.4481\n",
      "Epoch 590/600, Loss: 0.4483\n",
      "Epoch 591/600, Loss: 0.4486\n",
      "Epoch 592/600, Loss: 0.4488\n",
      "Epoch 593/600, Loss: 0.4486\n",
      "Epoch 594/600, Loss: 0.4487\n",
      "Epoch 595/600, Loss: 0.4484\n",
      "Epoch 596/600, Loss: 0.4484\n",
      "Epoch 597/600, Loss: 0.4483\n",
      "Epoch 598/600, Loss: 0.4484\n",
      "Epoch 599/600, Loss: 0.4483\n",
      "Epoch 600/600, Loss: 0.4482\n"
     ]
    }
   ],
   "source": [
    "#Creating neural network model\n",
    "from mlmodule.layers import Dense\n",
    "from mlmodule.models import Sequential\n",
    "from mlmodule.activations import *\n",
    "from mlmodule.optimizers import Adam\n",
    "from mlmodule.losses import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = relu))\n",
    "model.add(Dense(32, activation = relu))\n",
    "model.add(Dense(16, activation = relu))\n",
    "model.add(Dense(8, activation = relu))\n",
    "model.add(Dense(1, activation = sigmoid))\n",
    "\n",
    "#Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=binary_crossentropy)\n",
    "\n",
    "#Training the model\n",
    "model.fit(X_train, y_train, epochs=600, batch_size=64)\n",
    "\n",
    "#Predicting the output\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique value counts:  (array([0, 1]), array([12055,  2371]))\n"
     ]
    }
   ],
   "source": [
    "#Unique values\n",
    "print(\"Unique value counts: \", np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.52717105491149\n"
     ]
    }
   ],
   "source": [
    "#Calculating the accuracy\n",
    "accuracy = np.mean(binary_predictions == y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNetworkFromScratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
